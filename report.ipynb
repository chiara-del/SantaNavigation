{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobile Robotics Project Report - Santa Navigation\n",
    "### **Group 31:** Chiara Delvecchio, Camille Dorster, Junseo Um, Jérôme Courdacy\n",
    "\n",
    "**Task Distribution**:  \n",
    "- Chiara: Local Avoidance, Vision, Motion Control\n",
    "- Camille: Kalman filter, Path Planning, Local Avoidance\n",
    "- Junseo: Kalman filter\n",
    "- Jérôme: Vision, Local Avoidance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction: The Mission\n",
    "Our project is named **\"SantaNavigation\"**, born from a simple yet urgent necessity: someone had to help Santa Claus deliver gifts efficiently!\n",
    "In this scenario, our Thymio robot acts as Santa's high-tech sleigh. Its mission is to navigate autonomously from a starting point (the North Pole) to a destination (the Chimney/Goal) while carrying out its delivery duties.\n",
    "However, the journey is fraught with perils. To succeed, our system must overcome three main challenges:\n",
    "- **The Mountains (Global Obstacles):** The sleigh must plan a path around static geographic features that are known and mapped.\n",
    "- **The \"Ryanair Planes\" (Local Obstacles):** The sleigh must reactively dodge unexpected, unmapped dynamic obstacles (like low-flying budget airlines) that appear suddenly in its path.\n",
    "- **Vehicle Control:** We must ensure the sleigh doesn't get lost and handles smoothly, filtering noisy signals to provide a comfortable ride for the gifts.\n",
    "\n",
    "# 2. The Hardware Setup\n",
    "#### 2.1 The \"Eye in the Sky\" (Vision System)\n",
    "To give Santa a complete view of the world, we utilized an *AUKEY Stream Series 1080P HD Webcam*.\n",
    "- **Placement:** The camera is mounted overhead, positioned as perpendicular to the ground as possible (Nadir view).\n",
    "- **Role:** This sensor is the core of our Global Perception. Its job is to construct the Global Map, identify the obstacles, localize the Thymio, and pinpoint the target (Goal Position).\n",
    "\n",
    "#### 2.2 The Sleigh (Thymio)\n",
    "The vehicle used is the *Thymio*, a differential drive mobile robot. It is equipped with horizontal proximity sensors which detect the local obstacles (Ryanair planes mounted on a box) that the overhead camera will miss by design.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Software Architecture: Modular Design\n",
    "The implementation strategy for \"SantaNavigation\" was driven by the principle of **modularity**. To ensure the main control loop (main.py) remained clean, we delegated all complex mathematical operations, hardware interfaces, and image processing tasks to specialized utility modules (\"utils\").\n",
    "\n",
    "\n",
    "This separation allows each module to focus on its specific domain, making the code more robust and maintainable. Below is a breakdown of our utility modules and their responsibilities.\n",
    "\n",
    "**3.1 vision_utils.py**\n",
    "This module encapsulates all Computer Vision logic using a class-based approach (Vision class). It takes care of: camera management, automatic calibration, computation of the Homography Matrix, creation of the top-down map, ArUco detection, obstacles detection and finally visualization.\n",
    "\n",
    "**3.2 pathplanning_utils.py**\n",
    "This module is responsible for finding the best route. It deals purely with geometry and graph theory, independent of the robot's hardware. We use  Visibility Graph strategy. Then we use A* algorithm to find the shortest path through the visibility graph. Moreover in this utils file we also implement a Path Smoothing strategy that includes logic to simplify the path by removing unnecessary collinear waypoints, ensuring the robot doesn't make redundant stops.\n",
    "\n",
    "**3.3 control_utils.py**\n",
    "This module groups motor control, path following, and local avoidance into a compact control layer used throughout our system (look at section X).\n",
    "\n",
    "**3.4 ekf_pose.py**\n",
    "This module contains the class EKFPose which has all the functions and variables needed to execute the EKF. It takes care of computation of new position with EKF and visualization function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Environment Design & Computer Vision Strategies\n",
    "\n",
    "**Setup Overview:** The environment was designed to simplify reliable vision-based detection. A solid green surface was used as the background to maximize contrast, while obstacles were colored in a distinct red. ArUco markers were placed to localize the Thymio robot, the goal, and to calibrate the camera system. In the following sections, we describe in detail the calibration, localization, and obstacle detection strategies implemented to enable accurate navigation.\n",
    "\n",
    "**ArUcos** - For precise and reliable localization of the robot, the goal, and for calibrating the camera system, we utilized ArUco Markers.\n",
    "What is an ArUco Marker? An ArUco marker is a synthetic square marker composed of a wide black border and an inner binary matrix (a pattern of black and white squares). The black border facilitates its fast detection in the image and the binary codification allows its identification and the application of error detection and correction techniques. \n",
    "How we used them (Indices 0-5): We assigned specific roles to specific Marker IDs to automate the entire setup:\n",
    "1. **Automatic Calibration (IDs 2, 3, 4, 5):** We placed four markers at the corners of our arena. The system automatically detects these four corners to perform a perspective transform. See Homography section for more information.\n",
    "\n",
    "2. **Measuring dimensions:** By measuring the distance between two ArUco at the ends of the green background and comparing it with a known measurement (the size in cm of the background) we can understand the px/cm ratio and consequently calculate the radius of the Thymio in px.\n",
    "\n",
    "3. **The Sleigh (ID 0):** A specific marker is attached to the Thymio. This allows the system to calculate the robot's (x, y) position and its orientation (theta). Knowing the robot's heading is essential for our path-following control logic. (Note: in the main system we do not rely directly on these raw camera measurements of position and orientation that are later refined using a Kalman Filter, as discussed in Section X).\n",
    "\n",
    "4. **The Goal (ID 1):** Another marker designates the target destination (x,y).\n",
    "\n",
    "**Why ArUco?** ArUco markers were chosen because they are inexpensive, easy to deploy, and provide fast and reliable pose estimation using only a camera. \n",
    "\n",
    "#### 4.1 Vision initialization:\n",
    "\n",
    "The goal of the initialization is to provide the Vision object with all the necessary attributes to perform all the tasks needed. We provide constants like the `camera_index` and camera frame dimensions `cam_width` and `cam_height`. We also define the dimensions of the post-calibration frame, corresponding to the `map_width`and `map_height` of our robot's map in pixels. We also provide information about the ArUco indices corresponding to the thymio and goal using `thymio_marker_id` and `goal_marker_id`. Finally we pass it the transformation matrix path in `matrix_file_path`.\n",
    "\n",
    "During the initialization of the Vision system, we connect to the webcam using the index provided and try to load it if it already exists. If it doesn't, we compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvision_utils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mvu\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import vision_utils as vu\n",
    "import pathplanning_utils as pu\n",
    "import control_utils as cu\n",
    "import asyncio\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tdmclient import ClientAsync\n",
    "from ekf_pose import EKFPose\n",
    "\n",
    "\n",
    "# --- CONFIG ---\n",
    "CFG = {\n",
    "    \"CAM\": 0, \"RES\": (1920, 1080), \"MAP\": (1000, 700), \"MTX\": \"calibration_matrix.npy\",\n",
    "    \"IDS\": (0, 1), \"AREA\": 100,\n",
    "    \"THRESH\": (600, 1000), \"GAIN\": 0.06, \"BLIND\": 0.5, \"KIDNAP\": 60\n",
    "}\n",
    "\n",
    "raw_frame = cv2.imread(\"raw_frame.png\")\n",
    "vision = vu.Vision(CFG[\"CAM\"], *CFG[\"RES\"], *CFG[\"MAP\"], CFG[\"MTX\"], *CFG[\"IDS\"], CFG[\"AREA\"], test_frame = raw_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Perspective Transformation: Homography\n",
    "\n",
    "In order to work only with the the map and not the whole raw camera frame, we need to perform a transformation called homography. Homography allows us to, using a homography matrix, transform our raw frame from the camera to a map-specific frame. This allows us to navigate our map with coherent pixels to cms equivalences.\n",
    "\n",
    "In order to compute the homography matrix, we need to find four points in our raw frame which we know the raw coordinates of, as well as their post-transformation coordinates. In order to do this, we use four corner AruCos. We get their raw coordinates, corresponding to the coordinates of the corners of our map. Since these points ar the map corners, we can easily obtain their post-transformation coordinates using the transformed frame width and height. The map will then be 1000 x 700 pixels, corresponding to 1 pixel = 1 $mm^2$.\n",
    "\n",
    "During the calibration, the camera system detects the corner ArUcos automatically but a user confirmation is required to make sure that the obtained corner coordinates are correct.\n",
    "\n",
    "Once computed, the matrix is stored at the indicated path and can be reused while the camera and map setup doesn't change. We can now obtain the post-transformation warped frame that we will use in the rest of the project. Now, we are ready to detect the global obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(raw_frame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.title('Raw Frame (Before)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_frame = vision.get_warped_frame(raw_frame)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(warped_frame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.title('Warped Frame (After)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Global Obstacle Detection\n",
    "\n",
    "##### 1-Color Thresholding and Mask  \n",
    "To ensure robust obstacle detection we engineered the environment with high chromatic contrast and we use HSV color thresholder to isolate the desired lower/upper **HSV (Hue, Saturation, Value)** color range. As said before, we used:\n",
    "- **Background:** A solid Green surface.\n",
    "- **Obstacles:** Objects of a distinct Red color.\n",
    "\n",
    "**Why HSV?** Among simple and fast alternatives such as BGR and grayscale, HSV stands out as the most reliable choice for this task. BGR is often unstable, as a “red” object may appear as a dark maroon under shadow, causing significant variation across all three channels. Similarly, a grayscale thresholding approach was excluded because it discards chromatic information, making red and green objects potentially indistinguishable when they share similar luminance or are affected by uneven lighting.\n",
    "\n",
    "\n",
    "In contrast, HSV preserves color information more robustly: the hue remains relatively constant even when brightness fluctuates, and the red objects in our scene are highly saturated compared to the black-and-white ArUco marker. This makes red–green separation significantly more stable against shadows and illumination changes.\n",
    "\n",
    "\n",
    "*Limitations:* It is important to mention that despite the robustness of HSV, the specific threshold values for the filter are still dependent on the ambient light. Drastic changes in room lighting may require re-tuning of these thresholds.\n",
    "\n",
    "\n",
    "To address this, we developed a dedicated Python tool, *hsv_tuner.py*, which allows manual adjustment of the threshold ranges in real time. This tool enables us to tweak the HSV values and immediately visualize their effect on the filtered image, making re-tuning faster and more controlled.\n",
    "Once the red regions are isolated through HSV thresholding, we apply a Gaussian Filter and a morphological opening to remove noise and obtain a clean binary mask. Since the Thymio robot also emits red light from its LEDs, we explicitly exclude it from detection by drawing a circular mask at the estimated robot position, with a radius matching its physical size.\n",
    "\n",
    "##### 2-Polygonal Approximation\n",
    "The resulting mask is then converted into vector shapes through *_mask_to_polygons()*, which extracts contours, filters out small artifacts, and approximates them into polygons. Finally, *_expand_and_merge_polygons()* enlarges these polygons by the robot radius and merges overlaps using geometric buffering, producing smooth obstacle boundaries that guarantee safe clearance for navigation. \n",
    "These design choices serve two practical purposes. First, expanding each polygon by the radius of the robot allows us to treat the Thymio as a single point during path planning since the free space already accounts for its physical footprint. Second, merging nearby expanded polygons is particularly beneficial for the efficiency of our visibility graph planner: the algorithm’s complexity increases with the number of vertices, so reducing multiple close obstacles into a single unified polygon decreases graph size, resulting in faster computation and more scalable navigation performance.\n",
    "\n",
    "**Implementation note:** Obstacle detection is performed in the main script but **outside the main loop**, because the global obstacles remain static throughout the task. Running detection in real time is unnecessary and could even be problematic: if the robot accidentally occludes an obstacle from the camera’s perspective, the detected obstacle would be altered, potentially modifying the planned path at runtime, which is undesirable for stable navigation.\n",
    "\n",
    "\n",
    "**Technical Note:** Before building the obstacle map, we perform a 2-second **camera warm-up** to allow auto-exposure and white-balance to stabilize and to flush initial buffered frames. This prevents the first detection cycle from misinterpreting the scene (e.g., overexposed frames), avoiding incorrect maps that could mislead the planner.\n",
    "\n",
    "Right after warm-up, obstacle detection is computed using the median of multiple (5) consecutive frames rather than a single snapshot. Taking the pixel-wise median reduces sensor noise and removes transient artifacts, ensuring that only persistent red regions are classified as obstacles. This produces a clean, reliable static map for navigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obstacles, obstacle_mask = vision.detect_obstacles(frame = warped_frame)\n",
    "obstacles_frame = warped_frame.copy()\n",
    "vision.draw(frame = obstacles_frame, obstacles = obstacles)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(14,6))\n",
    "axs[0].imshow(obstacle_mask, cmap='gray')\n",
    "axs[0].set_title(\"Obstacle Mask\")\n",
    "axs[0].axis('off')\n",
    "axs[1].imshow(cv2.cvtColor(obstacles_frame, cv2.COLOR_BGR2RGB))\n",
    "axs[1].set_title(\"Warped Frame with Detected Obstacles\")\n",
    "axs[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3 - Thymio and Goal Detection\n",
    "Here we get the position of both the Thymio and the Goal using ArUco detection. The pose for the Goal is just (x,y) while for the Thymio we also get the angle theta. This is just the raw position taken from the camera that will be needed after for filtering and global navigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thymio_pose = vision.get_thymio_pose(warped_frame)\n",
    "goal_pos = vision.get_goal_pos(warped_frame)\n",
    "pose_frame = warped_frame.copy()\n",
    "vision.draw(frame = pose_frame, pose=thymio_pose, goal=goal_pos)\n",
    "plt.imshow(cv2.cvtColor(pose_frame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.title('Warped Frame with Thymio and Goal Detected')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* We delete the calibration matrix to recomputed for the main run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision.delete_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Path Planning with Visibility Graph and A* — Detailed Rationale and Validation\n",
    "\n",
    "#### 5.1 Problem Formulation and Configuration-Space Modeling\n",
    "\n",
    "##### Robot and Clearance\n",
    "We treat the robot as a point by inflating each obstacle polygon by at least the robot radius (configuration space). This “growing” guarantees that any straight segment planned between two points in free space is collision-free for the real robot. It also prevents shortest paths from legally “grazing” uninflated corners that would be unsafe in practice.  \n",
    "\n",
    "In our code, we assume polygons are already grown before planning; feasibility is then enforced downstream by geometric tests in `visible(...)` and `segment_hits_polygon(...)`.\n",
    "\n",
    "##### Start/Goal Validity\n",
    "Before adding edges, we check that neither start nor goal lies inside a grown polygon using `point_in_polygon(...)`; otherwise, planning is rejected.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.2 Visibility-Graph Construction (Why and How)\n",
    "\n",
    "**Why Visibility Graphs**\n",
    "In polygonal maps with grown obstacles, Euclidean shortest paths are piecewise-linear and run along straight segments that are “tangent” to obstacle hulls. Therefore, the shortest route between two points lies on the visibility graph:  \n",
    "\n",
    "- **Nodes**: start, goal, and obstacle vertices  \n",
    "- **Edges**: connect node pairs whose segment lies entirely in free space  \n",
    "\n",
    "Compared to a grid, this approach preserves geometric optimality with many fewer states and avoids “staircasing.”\n",
    "\n",
    "##### How We Build It\n",
    "- **Nodes**: `build_visibility_graph(...)` collects start, goal, and all vertices; it deduplicates them by rounding to avoid numerical duplicates.\n",
    "- **Edge Feasibility**: For each pair `(a, b)`, `visible(...)` checks two conditions:  \n",
    "  1. Endpoints are outside all polygons (`point_in_polygon(...)`)  \n",
    "  2. Segment `a-b` does not intersect any polygon edge (`segments_intersect(...)` via `segment_hits_polygon(...)`)\n",
    "- **Shared-Vertex Policy**: We allow a segment to touch a polygon exactly at a shared vertex (`allow_shared_endpoint=True`). This preserves valid shortest paths that pivot around corners after obstacle growing while still forbidding edge crossings. All other contacts count as collisions.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.3 Geometric Robustness (Floating-Point and Near-Degenerate Cases)\n",
    "- **Orientation and Collinearity**: We use `orient(...)` with a small epsilon to classify left/right/collinear reliably.  \n",
    "- **On-Segment Checks**: `on_segment(...)` decides if a collinear point lies on a segment.  \n",
    "These tolerances avoid false intersections when vertices are nearly collinear or coordinates are rounded.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.4 Search Objective and the A* Algorithm\n",
    "\n",
    "**Objective**\n",
    "Minimize total Euclidean path length \\(L\\) over the visibility graph.\n",
    "\n",
    "##### State Metrics\n",
    "- \\(g(u)\\): best-known cost from start to node \\(u\\)  \n",
    "- \\(h(u)\\): Euclidean distance from \\(u\\) to goal  \n",
    "- \\(f(u) = g(u) + h(u)\\): priority in the min-heap (`heapq`)\n",
    "\n",
    "##### Expansion Loop\n",
    "1. Pop the node \\(u\\) with smallest \\(f\\).  \n",
    "2. If \\(u\\) is the goal, reconstruct the path through parent pointers (`parent[v] = u` when updating `g(v)`).  \n",
    "3. Otherwise, for each neighbor \\(v\\) with edge weight \\(w\\):  \n",
    "   - Compute `ng = g(u) + w`  \n",
    "   - If \\(v\\) is unseen or `ng < g(v)`, update `g(v)`, set the parent, and push `f(v) = ng + h(v)`  \n",
    "\n",
    "##### Closed Set\n",
    "Once \\(u\\) is popped and added to closed, it is not re-relaxed (Euclidean \\(h\\) is consistent, so this is correct).\n",
    "\n",
    "##### Efficiency\n",
    "A* explores far fewer nodes than Dijkstra (with \\(h \\equiv 0\\)) because the heuristic directs the search toward the goal without sacrificing optimality.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.5 Path Post-Processing and Interface\n",
    "\n",
    "**Waypoint Cleanup**\n",
    "`simplify_collinear(...)` removes strictly collinear intermediate nodes so waypoints mark only true direction changes. This does not alter total length and produces cleaner commands for the controller.\n",
    "\n",
    "**Pipeline Assembly**\n",
    "`plan_path(...)` assembles the pipeline: it builds the graph, runs A*, then simplifies and returns the waypoints and the total length.\n",
    "\n",
    "#### 5.6 Example\n",
    "\n",
    "In this example implementing our optimal path finder, we set up a line-based map with polygonal obstacles already grown (inflated by robot radius in real simulation). The start and goal are represented respectively by a green dot and a blue star.\n",
    "\n",
    "Results shown:\n",
    "The light-blue lines are all visible \"roads\" (edges) between nodes; the red line is the shortest path. Waypoints which are the nodes forming the shortest path as well as the path length are given. The path stays in free space and does not cross obstacles thanks to obstacle growing and the visibility test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m safety \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00\u001b[39m  \u001b[38;5;66;03m# tu peux mettre 0.02 pour ajouter une marge\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Planification\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m path, total_len\u001b[38;5;241m=\u001b[39m pu\u001b[38;5;241m.\u001b[39mplan_path(start, goal, polygons, safety\u001b[38;5;241m=\u001b[39msafety)\n\u001b[0;32m     16\u001b[0m graph \u001b[38;5;241m=\u001b[39m pu\u001b[38;5;241m.\u001b[39mbuild_visibility_graph(start, goal, polygons, safety\u001b[38;5;241m=\u001b[39msafety)  \u001b[38;5;66;03m# for plotting edges\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLongueur de chemin: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_len\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pu' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ---------- Scène de test ----------\n",
    "# Obstacles déjà “gonflés” (mètres). Deux polygones convexes pour la démo.\n",
    "polygons = [\n",
    "    [(0.6, 0.2), (0.9, 0.2), (0.9, 0.5), (0.6, 0.5)],      # rectangle\n",
    "    [(0.3, 0.7), (0.55, 0.55), (0.8, 0.75), (0.55, 0.9)]   # quadrilatère\n",
    "]\n",
    "start = (0.1, 0.1)\n",
    "goal  = (0.95, 0.95)\n",
    "safety = 0.00  # tu peux mettre 0.02 pour ajouter une marge\n",
    "\n",
    "# Planification\n",
    "path, total_len= pu.plan_path(start, goal, polygons, safety=safety)\n",
    "graph = pu.build_visibility_graph(start, goal, polygons, safety=safety)  # for plotting edges\n",
    "\n",
    "print(f\"Longueur de chemin: {total_len:.3f} m\")\n",
    "print(\"Waypoints:\")\n",
    "for p in path or []:\n",
    "    print(f\"  {p}\")\n",
    "\n",
    "# ---------- Visualisation ----------\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "# Obstacles\n",
    "for P in polygons:\n",
    "    xs = [p[0] for p in P] + [P[0][0]]\n",
    "    ys = [p[1] for p in P] + [P[0][1]]\n",
    "    ax.fill(xs, ys, color='gray', alpha=0.4, edgecolor='k')\n",
    "\n",
    "# Graphe de visibilité (toutes les arêtes)\n",
    "for u, nbrs in graph.items():\n",
    "    for v, _ in nbrs:\n",
    "        # tracer chaque arête une seule fois\n",
    "        if u < v:\n",
    "            ax.plot([u[0], v[0]], [u[1], v[1]], color='lightblue', linewidth=0.8, zorder=1)\n",
    "\n",
    "# Chemin final\n",
    "if path:\n",
    "    px = [p[0] for p in path]\n",
    "    py = [p[1] for p in path]\n",
    "    ax.plot(px, py, '-r', linewidth=2.5, label='Chemin', zorder=3)\n",
    "    ax.scatter(px, py, c='r', s=20, zorder=4)\n",
    "\n",
    "# Start/Goal\n",
    "ax.scatter([start[0]], [start[1]], c='green', s=60, marker='o', label='Start', zorder=5)\n",
    "ax.scatter([goal[0]], [goal[1]], c='blue',  s=60, marker='*', label='Goal', zorder=5)\n",
    "\n",
    "ax.set_xlim(0.0, 1.05)\n",
    "ax.set_ylim(0.0, 1.05)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.2)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. LOCAL AVOIDANCE\n",
    "While the vision-based global map allows the robot to plan an optimal path around static obstacles, real environments are unpredictable. A new object may appear on the floor, or the robot may encounter something unseen by the camera. To handle these situations, we implemented a two-state Finite State Machine (FSM) that governs the navigation strategy in real time.\n",
    "\n",
    "The first state (**Global Navigation**) is active during normal operation. If  max(prox[:5]) is remain below 1000, the robot follows the planned A* path, using the PathFollower controller to steer from one waypoint to the next.\n",
    "If instead an unexpected obstacle comes dangerously close (any proximity sensor > 1000), the FSM switches to the second state (**Local Avoidance**) where the robot relies on reactive behavior to move away from danger.\n",
    "\n",
    "To avoid unstable oscillations between behaviours, the system uses hysteresis: entry into avoidance occurs at 1000, whereas a safe return to global navigation is only allowed once readings fall below 600. This gap ensures smooth, decisive transitions rather than rapid back-and-forth switching. When switching back to global navigation we also force the system to recalculate the optimal global path. This choice avoids taking suboptimal paths caused by the local navigation routine.\n",
    "\n",
    "This hybrid approach allows the robot to follow its planned route efficiently, while still responding to unforeseen obstacles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![descrizione immagine](local_avoidance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. CONTROL\n",
    "The control layer serves as the bridge between high-level planning and low-level actuation, ensuring that the robot can both follow a predefined route and react to unexpected obstacles.\n",
    "\n",
    "#### 7.1 Hardware Abstraction Layer — ThymioController\n",
    "Motor commands are handled through the ThymioController class in control_utils.py. ThymioController acts as a wrapper for motor commands, allowing the code to set wheel speeds cleanly through the node without interacting directly with hardware variables.\n",
    "Path execution is handled by the PathFollower class, which receives a sequence of waypoints and drives the robot through them using a proportional controller on the heading error. The controller checks when a waypoint is reached and automatically progresses to the next target, adjusting wheel speeds accordingly.\n",
    "#### 7.2 Global Navigation — Proportional Control (P-Controller)\n",
    "During normal operation, the robot follows the sequence of waypoints produced by the A* planner thanks to the PathFollower modules.\n",
    "Heading control is achieved using a P-controller, where the steering command is proportional to the angular deviation from the waypoint. \n",
    "\n",
    "**Why P-Controller?** The P-Controller is easy to tune because it has just one gain parameter Kp and it’s robust enough to drift.  By constantly recalculating the error relative to the current vision-based position, the controller naturally corrects for physical disturbances like wheel slippage or uneven terrain without needing complex odometry models.\n",
    "\n",
    "\n",
    "*Limitations:* We are aware that a small residual steady-state error may persist and that high gain values can cause overshoot or oscillation in tight curves. \n",
    "\n",
    "\n",
    "#### 7.3 Local Navigation\n",
    "If the proximity sensors detect an unexpected object, the FSM switches to Local Avoidance and the controller adopts a logic where stronger readings on one side increase the speed of the opposite motor, steering the robot away from obstacles reflexively.\n",
    "\n",
    "\n",
    "**Why do we choose this strategy?** This represents a good strategy because it has zero-latency response and a lightweight computation that is ideal alongside vision processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Extended Kalman Filter (EKF) for Robot Pose Estimation\n",
    "\n",
    "#### 8.1 State Representation\n",
    "\n",
    "We represent the robot's state as a vector of three variables:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "\\theta\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "\n",
    "- \\(x, y\\) are the robot's position in mm,  \n",
    "- $\\theta$ is the robot's orientation in radians.\n",
    "\n",
    "The robot receives odometry measurements from the wheel encoders:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} = \n",
    "\\begin{bmatrix}\n",
    "v \\\\\n",
    "\\omega\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $v = \\frac{v_L + v_R}{2}$ is the linear velocity (mm/s),  \n",
    "- $\\omega = \\frac{v_R - v_L}{L}$ is the angular velocity (rad/s),  \n",
    "- $v_L, v_R$ are the left and right wheel speeds,  \n",
    "- $L$ is the distance between the wheels.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.2 Prediction Step\n",
    "\n",
    "The EKF prediction propagates the state using the odometry-based motion model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_{k+1} &= x_k + v \\sin(\\theta_k) \\, \\Delta t \\\\\n",
    "y_{k+1} &= y_k - v \\cos(\\theta_k) \\, \\Delta t \\\\\n",
    "\\theta_{k+1} &= \\theta_k + \\omega \\, \\Delta t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\Delta t$ is the elapsed time since the last measurement.\n",
    "\n",
    "The Jacobian of the motion model with respect to the state is:\n",
    "\n",
    "$$\n",
    "G = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -v \\cos(\\theta) \\Delta t \\\\\n",
    "0 & 1 & -v \\sin(\\theta) \\Delta t \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The predicted covariance is computed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{k+1|k} = G \\mathbf{P}_k G^\\top + Q\n",
    "$$\n",
    "\n",
    "where \\(Q\\) is the **process noise covariance** representing the uncertainty in odometry:\n",
    "\n",
    "$$\n",
    "Q = \n",
    "\\begin{bmatrix}\n",
    "Q_x & 0 & 0 \\\\\n",
    "0 & Q_y & 0 \\\\\n",
    "0 & 0 & Q_\\theta\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 8.3 Update Step\n",
    "\n",
    "The EKF incorporates measurements from a **vision system** (Aruco marker detection) to correct the predicted state.  \n",
    "\n",
    "For position measurements:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_{pos} = \n",
    "\\begin{bmatrix}\n",
    "x_{meas} \\\\\n",
    "y_{meas}\n",
    "\\end{bmatrix}, \n",
    "\\quad\n",
    "H_{pos} =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For orientation measurements:\n",
    "\n",
    "$$\n",
    "z_\\theta = \\theta_{meas}, \n",
    "\\quad\n",
    "H_\\theta = \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The measurement residual is:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{z} - h(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "The Kalman gain is:\n",
    "\n",
    "$$\n",
    "K = \\mathbf{P}_{k+1|k} H^\\top (H \\mathbf{P}_{k+1|k} H^\\top + R)^{-1}\n",
    "$$\n",
    "\n",
    "The state and covariance are updated as:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{k+1} = \\mathbf{x}_{k+1|k} + K \\mathbf{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{k+1} = (I - K H) \\mathbf{P}_{k+1|k}\n",
    "$$\n",
    "\n",
    "The measurement noise covariance \\(R\\) is:\n",
    "\n",
    "$$\n",
    "R =\n",
    "\\begin{bmatrix}\n",
    "R_{posx} & 0 & 0 \\\\\n",
    "0 & R_{posy} & 0 \\\\\n",
    "0 & 0 & R_\\theta\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4 EKF Parameter Tuning\n",
    "\n",
    "##### 8.4.1 Tuning of parameters `Qx`, `Qy`, `Qθ` of the matrix Q\n",
    "\n",
    "The EKF prediction step uses wheel odometry to estimate the robot's pose between two measurements. For a differential-drive robot, the linear and angular velocities are obtained from the left and right wheel encoder speeds:\n",
    "\n",
    "$$\n",
    "v = \\frac{v_L + v_R}{2}, \\quad\n",
    "\\omega = \\frac{v_R - v_L}{L}\n",
    "$$\n",
    "\n",
    "and the pose is propagated according to:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k + v \\cos(\\theta_k) \\, dt, \\quad\n",
    "y_{k+1} = y_k + v \\sin(\\theta_k) \\, dt, \\quad\n",
    "\\theta_{k+1} = \\theta_k + \\omega \\, dt\n",
    "$$\n",
    "\n",
    "Odometry is inherently imperfect due to wheel slip, encoder quantization, and uneven wheel radii, which leads to a progressive divergence between the predicted pose and the true trajectory.\n",
    "\n",
    "These uncertainties are represented in the process noise matrix:\n",
    "\n",
    "\n",
    "$$\n",
    "Q = \n",
    "\\begin{bmatrix}\n",
    "Q_x & 0 & 0 \\\\\n",
    "0 & Q_y & 0 \\\\\n",
    "0 & 0 & Q_\\theta\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "The tuning of these parameters determines how much the EKF trusts its internal motion model compared to external measurements.\n",
    "\n",
    "##### 8.4.2 Tuning Procedure\n",
    "\n",
    "1. **Initial conservative values**  \n",
    "   Small values of `Qx`, `Qy`, and `Qθ` were first used so the filter strongly trusts odometry. This makes the prediction smooth but prevents the filter from reacting quickly to measurement corrections.\n",
    "\n",
    "2. **Observation of drift in real experiments**  \n",
    "   Several runs were performed where the robot moved along straight lines and rotations. The raw odometry trajectory was compared with the external vision-based pose. The drift rate was measured and used as an empirical baseline.\n",
    "\n",
    "3. **Incremental increase of `Qx` and `Qy`**  \n",
    "   - These parameters were gradually increased until the EKF became responsive enough to correct lateral and longitudinal drift as soon as new measurements arrived, without overreacting.  \n",
    "   - Too-small values caused the filter to “ignore” vision corrections and slowly drift with odometry.  \n",
    "   - Too-large values resulted in noisy corrections and small oscillations around the vision measurement.  \n",
    "   - The selected values represent the smallest amount of noise that makes the filter consistently converge back to the vision-based pose after each prediction step.\n",
    "\n",
    "4. **Tuning of `Qθ`**  \n",
    "   - Gyro drift was evaluated by observing repeated in-place rotations.  \n",
    "   - `Qθ` was increased until the EKF corrected heading errors quickly while maintaining a stable orientation estimate.  \n",
    "   - Too small a value caused slow correction of heading drift, while too large a value produced jittery orientation estimates.\n",
    "\n",
    "The resulting process noise values reflect the actual uncertainty level of the robot’s odometry: the EKF trusts odometry for short-term motion, but allows external measurements (vision) to dominate whenever they reveal drift. This tuning results in a filter that is both stable and reactive, providing an accurate real-time pose estimate.\n",
    "\n",
    "---\n",
    "\n",
    "##### 8.4.3 Tuning of `R`\n",
    "\n",
    "The measurement noise matrix `R` models the uncertainty of the pose estimates provided by the vision system. In our setup, the robot carries an **Aruco marker**, detected by a webcam to measure its position `(x, y)` and orientation `θ`:\n",
    "\n",
    "$$\n",
    "R =\n",
    "\\begin{bmatrix}\n",
    "R_{posx} & 0 & 0 \\\\\n",
    "0 & R_{posy} & 0 \\\\\n",
    "0 & 0 & R_\\theta\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The tuning was performed using **static measurements only**:\n",
    "\n",
    "1. The robot was placed stationary in front of the camera, and the ArUco marker was repeatedly detected.  \n",
    "2. The variance of the measured `x`, `y`, and `θ` values was computed and directly used as `Rposx`, `Rposy`, and `Rθ`. This captures the intrinsic noise of the camera detection system due to pixel jitter, lens distortions, and marker corner detection errors.\n",
    "\n",
    "### Effect of R values on EKF behavior\n",
    "\n",
    "- **Smaller R values** → the EKF trusts the camera measurements more. This results in quicker corrections of any odometry drift, but can make the state estimate react strongly to tiny pixel-level fluctuations, producing jittery or unstable estimates.  \n",
    "- **Larger R values** → the EKF trusts the odometry more and relies less on the camera. Corrections from vision are smoother but slower, so odometry drift persists longer before being corrected.\n",
    "\n",
    "By using the measured static variances, the filter balances stability and responsiveness: it trusts the ArUco detection enough to correct odometry errors while ignoring small detection fluctuations.\n",
    "\n",
    "#### 8.5 EKF Usage in the Main Program\n",
    "\n",
    "In the main loop, the EKF estimates the robot's pose `[x, y, θ]` by combining **wheel odometry** and **ArUco vision measurements**:\n",
    "\n",
    "1. **Predict step (odometry)**  \n",
    "\n",
    "Wheel encoder speeds are converted to linear and angular velocities:\n",
    "\n",
    "$$\n",
    "v = 0.5 \\cdot (s_L + s_R) \\cdot \\text{SPEED\\_TO\\_MMS}, \\quad\n",
    "\\omega = -(s_R - s_L) \\cdot \\text{omega\\_scale}\n",
    "$$\n",
    "\n",
    "Then the EKF propagates the state forward:\n",
    "\n",
    "u = np.array([v_meas, omega_meas])  \n",
    "ekf.predict(u)\n",
    "\n",
    "2. **Update step (ArUco measurements)**\n",
    "\n",
    "If the robot is detected by the camera, the measured position and orientation are use to correct EKF estimate:\n",
    "\n",
    "ekf.update_pos(x_mm, y_mm)  \n",
    "ekf.update_theta(theta_rad)\n",
    "\n",
    "3. **Retrieve the filtered state**\n",
    "The updated state is used for planning and control:  \n",
    "x_hat, P_hat = ekf.get_state()\n",
    "\n",
    "#### 8.6 Testing\n",
    "To be sure that the filter was working correctly we plot the EKF predicted position and the standard deviation in x on a straight line path. What we did was try to see how the standard deviation of the predicted position changes when we hide the camera. This allows us to see that the prediction uncertainty grows when the camera input is missing. Moreover, when we get the camera input back, we notice a correction in x position. You can see the resulting plot below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![descrizione immagine](kalman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7 Summary\n",
    "\n",
    "- The EKF combines **odometry prediction** and **vision updates**.  \n",
    "- Tuning Q and R balances **responsiveness vs stability**.  \n",
    "- The resulting filter provides **accurate real-time pose estimates** even in the presence of sensor noise and odometry drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. System Logic and Execution Flow\n",
    "The following part orchestrates the flow of data between perception, state estimation, planning, and control modules within a continuous asynchronous loop.\n",
    "\n",
    "#### 9.1 Initialization Phase\n",
    "Before the mission begins, the system performs a critical setup sequence:\n",
    "\n",
    "1. **Hardware Connection:** The script connects to the Thymio via tdmclient, locks the node, and subscribes to motor speed variables (motor.left.speed, motor.right.speed) required for odometry.\n",
    "2. **Vision & EKF Setup:**\n",
    "- The Vision class is initialized (camera setup, matrix loading as seen before).\n",
    "- The EKFPose filter is instantiated with specific covariance matrices (Q for process noise, R for measurement noise) tuned to the robot's physical characteristics.\n",
    "3. **Static Mapping:** The system performs a \"warm-up\" to stabilize the camera, then detect static red obstacles and generate the polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import vision_utils as vu\n",
    "import pathplanning_utils as pu\n",
    "import control_utils as cu\n",
    "import asyncio\n",
    "import time\n",
    "import math\n",
    "from tdmclient import ClientAsync\n",
    "from ekf_pose import EKFPose   # <- classe ci-dessus\n",
    "\n",
    "# --- CONFIG ---\n",
    "CFG = {\n",
    "    \"CAM\": 0, \"RES\": (1920, 1080), \"MAP\": (1000, 700), \"MTX\": \"calibration_matrix.npy\",\n",
    "    \"IDS\": (0, 1), \"AREA\": 100,\n",
    "    \"THRESH\": (600, 1000), \"GAIN\": 0.06, \"BLIND\": 0.5, \"KIDNAP\": 60\n",
    "}\n",
    "\n",
    "#1) Connect to Thymio\n",
    "client = ClientAsync()\n",
    "try:\n",
    "    print(\"Waiting for Thymio...\")\n",
    "    node = await client.wait_for_node()\n",
    "    await node.lock()\n",
    "    print(\"Connected!\")\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"Connection Failed: {e}\")\n",
    "\n",
    "# Ensure we can read motor speeds\n",
    "await node.wait_for_variables({\"prox.horizontal\", \"motor.left.speed\", \"motor.right.speed\"})\n",
    "\n",
    "robot = cu.ThymioController(node)\n",
    "follower = cu.PathFollower(speed=100, gain=2.5)\n",
    "\n",
    "#2) Initialize Vision\n",
    "vision = vu.Vision(CFG[\"CAM\"], *CFG[\"RES\"], *CFG[\"MAP\"], CFG[\"MTX\"],\n",
    "                    *CFG[\"IDS\"], CFG[\"AREA\"])\n",
    "if not vision.cap:\n",
    "    await node.unlock()\n",
    "    raise SystemExit(\"Vision Error\")\n",
    "\n",
    "#3) EKF initialization (units: mm, mm/s, rad)\n",
    "mm_per_px = 10.0/(vision.px_per_cm)   # mm/px \n",
    "ekf = EKFPose()\n",
    "seeded = False\n",
    "\n",
    "# Speed conversions\n",
    "SPEED_TO_MMS = 0.33 #mm/s per Thymio unit\n",
    "b_mm = 95.0                          # track width (mm) à mesurer\n",
    "omega_scale = SPEED_TO_MMS / b_mm    # rad/s per (Thymio unit)\n",
    "\n",
    "#4) Obstacle Detection\n",
    "print(\"Camera Warmup (2 seconds)...\")\n",
    "warmup_end = time.time() + 2.0\n",
    "while time.time() < warmup_end:\n",
    "    vision.get_warped_frame()\n",
    "    await asyncio.sleep(0.01)\n",
    "\n",
    "print(\"Mapping static obstacles...\")\n",
    "obs_contours, mask = vision.detect_obstacles()\n",
    "if not obs_contours:\n",
    "    print(\"WARNING: No obstacles detected! Check lighting.\")\n",
    "planner_obs = [[tuple(pt[0]) for pt in cnt] for cnt in obs_contours]\n",
    "debug_view = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "cv2.putText(debug_view, \"Initial Map Snapshot\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "cv2.imshow(\"Map\", debug_view)\n",
    "cv2.waitKey(1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2 The Control Loop (The \"Brain\")\n",
    "The system enters a while True loop. Each iteration follows the Predict-Update-Plan-Act cycle:\n",
    "\n",
    "**Step 1: Perception & Prediction (Sensor Fusion)**\n",
    "This system uses sensor fusion:\n",
    "\n",
    "- **Proprioception (Odometry):** The script reads the real-time speed of the left and right wheels. Using the differential drive kinematic model, it calculates the linear velocity (v) and angular velocity (omega).\n",
    "- **EKF Prediction:** Even before the camera processes a frame, the EKF predicts the robot's new position based on the previous state and the wheel velocities. This ensures the robot has a pose estimate even if the camera is blocked (blind navigation).\n",
    "- **Visual Measurement:** The camera captures a frame. The Vision module detects the ArUco marker to find the \"measured\" pose (x, y, \\theta).\n",
    "- **EKF Update:**\n",
    "- - If the marker is visible, the EKF **updates** its predicted state using the visual measurement, correcting any drift accumulated by the wheels.\n",
    "- - If the marker is not visible (occlusion), this step is skipped, and the robot relies entirely on the prediction (Odometry).\n",
    "\n",
    "**Step 2: Goal & Planning**\n",
    "- The goal position is updated from the camera feed (Marker 1).\n",
    "- **Kidnapping Detection:** The system continuously checks the Cross-Track Error. It calculates the distance between the robot's current filtered position and the line segment it should be following. If this error exceeds a threshold (60 pixels), the system infers that the robot has been manually moved (\"kidnapped\") or pushed off course. This triggers an immediate re-planning event to generate a new path from the new location.\n",
    "\n",
    "**Step 3: Hybrid State Machine (Decision Making)**\n",
    "The decision logic is a Finite State Machine (FSM) with hysteresis, relying on the Filtered Pose:\n",
    "- **State 0: GLOBAL_NAV (Path Following)**\n",
    "- - Condition to enter: Proximity sensors < 600.\n",
    "- - Action: The PathFollower calculates steering commands based on the smooth, filtered EKF pose (pose_for_planner). This eliminates the \"jitter\" caused by raw camera noise.\n",
    "- **State 1: LOCAL_NAV (Obstacle Avoidance)**\n",
    "- - Condition to enter: Proximity sensors > 1000.\n",
    "- - Action: The robot reacts purely to proximity sensors using Braitenberg logic.\n",
    "\n",
    "**Step 4: Action & Visualization**\n",
    "- **Control:** Motor commands are sent to the robot.\n",
    "- **Visualization:**\n",
    "- - **Map/Path/Obstacles:** Standard visual overlays.\n",
    "- - **Covariance Ellipse:** EKF's uncertainty. A small ellipse indicates high confidence (camera active), while a growing ellipse indicates uncertainty (camera blocked, relying on odometry).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        frame = vision.get_warped_frame()\n",
    "        if frame is None:\n",
    "            break\n",
    "        prox = list(node[\"prox.horizontal\"])\n",
    "\n",
    "        # Build u = [v_meas, omega_meas] from wheels\n",
    "        sL = float(node[\"motor.left.speed\"])\n",
    "        sR = float(node[\"motor.right.speed\"])\n",
    "        v_meas = 0.5 * (sL + sR) * SPEED_TO_MMS           # mm/s\n",
    "        omega_meas = -(sR - sL) * omega_scale              # rad/s\n",
    "        u = np.array([v_meas, omega_meas], float)\n",
    "\n",
    "        # EKF predict with u\n",
    "        ekf.predict(u)\n",
    "\n",
    "        # Get Thymio pose from vision\n",
    "        pose = vision.get_thymio_pose(frame)\n",
    "\n",
    "        # EKF updates (position + theta)\n",
    "        if pose:\n",
    "            (px, py), angle_deg = pose\n",
    "            x_mm = px * mm_per_px\n",
    "            y_mm = py * mm_per_px\n",
    "            theta_rad = math.radians(angle_deg)\n",
    "        \n",
    "            if not seeded:\n",
    "                ekf.x[:] = [x_mm, y_mm, theta_rad]\n",
    "                seeded = True\n",
    "\n",
    "            ekf.update_pos(x_mm, y_mm)\n",
    "            ekf.update_theta(theta_rad)\n",
    "\n",
    "        # Planner/control pose (convert back to pixels)\n",
    "        if seeded:\n",
    "            x_hat, _ = ekf.get_state()\n",
    "            px_hat = int(x_hat[0] / mm_per_px)\n",
    "            py_hat = int(x_hat[1] / mm_per_px)\n",
    "            theta_hat = (math.degrees(x_hat[2]))%360\n",
    "            pose_for_planner = ((px_hat, py_hat), theta_hat)\n",
    "        else:\n",
    "            pose_for_planner = pose\n",
    "\n",
    "        #Get Goal Position from vision\n",
    "        goal = vision.get_goal_pos(frame)\n",
    "\n",
    "        # State machine (local avoidance)\n",
    "        max_prox = max(prox[:5])\n",
    "        if state == 0 and max_prox > CFG[\"THRESH\"][1]:\n",
    "            state = 1\n",
    "        elif state == 1 and max_prox < CFG[\"THRESH\"][0]:\n",
    "            state = 0\n",
    "            follower.path = None\n",
    "\n",
    "        # Planning\n",
    "        if pose_for_planner and goal:\n",
    "            needs_plan = False\n",
    "            if not follower.path:\n",
    "                needs_plan = True\n",
    "            elif follower.path and state == 0:\n",
    "                prev = max(0, follower.current_idx - 1)\n",
    "                target = follower.path[follower.current_idx]\n",
    "                if pu.check_kidnapping(pose_for_planner, target, follower.path, prev, CFG[\"KIDNAP\"]):\n",
    "                    print(\"Kidnapping detected -> Replan\")\n",
    "                    needs_plan = True\n",
    "\n",
    "            if needs_plan:\n",
    "                print(\"Planning...\")\n",
    "                path, _ = pu.plan_path(pose_for_planner[0], goal, planner_obs, safety=0.0)\n",
    "                \n",
    "                if path:\n",
    "                    follower.set_path(path)\n",
    "                    print(\"Path Found!\")\n",
    "                else:\n",
    "                    print(\"No Path.\")\n",
    "\n",
    "\n",
    "\n",
    "        # Control\n",
    "        if pose_for_planner:\n",
    "            if state == 0:\n",
    "                l, r, done = follower.get_command(pose_for_planner)\n",
    "                if done:\n",
    "                    print(\"Goal reached!\")\n",
    "                    follower.path = None\n",
    "                await robot.set_motors(l, r)\n",
    "            else:\n",
    "                l, r = cu.calculate_avoidance_commands(prox, 50, 1.8)\n",
    "                await robot.set_motors(l, r)\n",
    "        else:\n",
    "            await robot.stop()\n",
    "\n",
    "        # Visualization\n",
    "        # Main map Visualization\n",
    "        status_text = \"GLOBAL\" if (state == 0) else \"LOCAL AVOIDANCE\"\n",
    "        vision.draw(frame = frame,\n",
    "                    obstacles = obs_contours, \n",
    "                    pose = pose, \n",
    "                    kalman_pose = pose_for_planner, \n",
    "                    goal = goal, \n",
    "                    path = follower.path, \n",
    "                    path_idx = follower.current_idx, \n",
    "                    state_text = status_text,\n",
    "                    )\n",
    "        cv2.imshow(\"Map\", frame)\n",
    "\n",
    "        # Kalman Visualization\n",
    "        kalman_var = np.ones((400, 400, 3), dtype=np.uint8) * 255\n",
    "        cv2.namedWindow(\"KalmanVariance\", cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(\"KalmanVariance\", 400, 400)\n",
    "        if seeded:\n",
    "            ekf.draw_covariance_ellipse(kalman_var, mm_per_px)\n",
    "        cv2.imshow(\"KalmanVariance\", kalman_var)\n",
    "\n",
    "\n",
    "        await asyncio.sleep(0.01)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    await robot.stop()\n",
    "    await node.unlock()\n",
    "    vision.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This project offered us the opportunity to fully appreciate the complexity behind coordinating multiple components to achieve navigation. By experimenting hands-on, we were able to observe the strengths and weaknesses of each module and to understand how design choices directly influence overall performance. If there is one key takeaway, it is that no universal “perfect recipe” exists in robotics: every solution represents a balance between computational cost, hardware constraints, and the specific conditions of the environment. Ultimately, effective navigation emerges not from ideal components, but from thoughtful compromise and informed engineering decisions.\n",
    "\n",
    "In the end, we did it —> Santa reached the chimney safely, gifts intact, mission accomplished!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
